{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 2 point correlation funciton, growth of large scales strucutres, and Baryon Acoustic Oscillations\n",
    "\n",
    "## Authors\n",
    "Jeff McMahon, Lindsey Bleem, and Alex Drlica-Wagner\n",
    "\n",
    " *Dedicated in the memory of Jeff's dad Tim who was always curious, always adventurous, and proud of him no mater what he did.*\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "It is well established that the Universe began in a hot dense state and expanded thereafter.  The initial conditions of this state are broadly consistent with the predictions of the inflationary paradigm. This paradigm predicts a minuscule spectrum of density perturbations generated from quantum fluctuations in the first  $\\sim 10^{-32}$ s.  These density perturbations then evolve under the influence of gravitational and (counteracting) pressure forces: overdense regions collapse leading to regions of high pressure in the photon-baryon fluid and then subsequently expand owing to this pressure (and then subsequently recollapse, etc).  These oscillatory dynamics are acoustic oscillations. \n",
    "\n",
    "### Accoustic Oscillations (source Wayne Hu)\n",
    "\n",
    "<div>\n",
    "<img src=\"http://background.uchicago.edu/~whu/intermediate/basicoscil.gif\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "As the universe expands, it cools to the point where electrons and protons combine to form neutral hydrogen (recombination).  From this time onward the photons are no longer coupled to the baryons.  This is the time at which the CMB is emitted (redshift ~1100) and the imprint of these acoustic oscillations explains the hot and cold spots of the CMB.  In particular the first (and tallest) acoustic peak of the CMB corresponds to the angular scale (wavelength) at which matter has just collapsed to its maximum density.  At angular scales larger than this, the finite speed of sound in the photon-baryon fluid and the finite age of the universe limited the amount of collapse possible.   The physical scale of this peak is determined by the sound speed in the early universe and the time at which decoupling happens.  This is called the sound horizon.   Since the speed of sound and time of decoupling are known, the first peak represents a **standard ruler** and the measurement of its angular scale provides the most precise constraint that our universe is spatially flat. \n",
    "\n",
    "Decoupling also changes the evolution of the acoustic oscillations.  After this time, the photons no longer contribute pressure and the baryon fluid starts to collapse.  Thus the inflationary signal that gives rise to the CMB anisotropy also seeds the formation of large scale structures (LSS, hereafter).   Measurements of the spatial spectrum of LSS consequently provides a complementary constraint on cosmological parameters.  One key difference between the CMB and these structures is that we can observe them in three dimensions: as two spatial directions on the sky (e.g., Right Ascension and Declination on the celestial sphere), and---along the line of sight---redshift.  Mapping LSS provides an avenue to measure the growth of structure and trace its evolution.  \n",
    "\n",
    "The full spectrum is needed to harvest all possible information.  However, just as the CMB has peaks which represent special, and easy to understand features, so does the spectrum of LSS.  The relevant feature arises from Baryon Acoustic Oscillations (BAO) as follows.  At the time of decoupling the sound horizon gives a scale with a maximum overdensity.  Upon decoupling, with no photon pressure to resist further collapse, structures grow at all scales. However, since the scale corresponding to the sound horizon is the most overdense at this time, it is expected to have an enhancement of collapsed structures (galaxies, gas, etc.).  It constitutes an additional standard ruler for cosmological measurements.   It can be observed by tracing the neutral hydrogen at high redshift, or by tracing the overdensity of galaxies at lower redshifts.  By observing the BAO in multiple redshift slices, one can reconstruct the angular diameter distance as a function of redshift.  This provides a sensitive probe of dark energy, neutrino masses, the Hubble constant, and other cosmological parameters. \n",
    "\n",
    "Artist rendition of BAO signature imprinted in galaxy distribution (Zosia Rostomian, LBL)\n",
    "<div>\n",
    "<img src=\"https://newscenter.lbl.gov/wp-content/uploads/sites/2/BOSS-BAO.jpg\" width=\"400\"/>\n",
    "</div>\n",
    "The BAO feature in galaxy surveys was first detected with spectroscopic measurements of galaxies observed at optical wavelengths by SDSS and 2dFGRS. \n",
    "Since then multiple teams have confirmed and improved these measurements.  Notably, DESI is currently in the process of obtaining over 30 million galaxy and quasar redshifts with the goal of making the most ambitious measurement to date.   \n",
    "\n",
    "Just as the first peak is measured by computing the angular power spectrum of the CMB, the BAO are measured by computing the matter power spectrum or a related quantity---the 2 point correlation function.  It is important to note that the 2 point correlation evolves with redshift and contains much more information beyond the BAO feature. For example the evolution of the spectrum traces the evolution and growth of structure.\n",
    "\n",
    "\n",
    "## Notebook Goals\n",
    "In this notebook you will gain hands-on experience working with spectroscopic data to measure statistical measurements of galaxy clustering by calculating the angular two-point (2pt) correlation function.  You will develop and apply a correlation function analysis to galaxies from the SDSS spectroscopic sample.  This tutorial follows an analysis of early SDSS data by [Connolly et al. 2001](https://arxiv.org/abs/astro-ph/0107417), and, by the end, you will have produced a figure similar to their [Figure 1](https://arxiv.org/pdf/astro-ph/0107417.pdf#page=7).  We will discuss extensions to this simple analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Standard python imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Used\n",
    "The data will be drawn from the Sloan Digitial Sky Survye (SDSS) Data release 16 (DR16).  These data can be queried here http://skyserver.sdss.org/dr16/en/tools/search/SQS.aspx.  We have downloaded three tables giving redshfit, ra, dec, and color inforamtion for objects classified as galaxies over a 5x5 degree region. The three sets span the redshift ranges of $0.08<𝑧<0.12$, $0.4<𝑧<0.5$ and $0.6<𝑧<0.9$.  The region is from RA 0 to 5 degrees and Dec. 0 to 5 degrees.  For ease of use we have downloaded tables containing these data for you. The columns of these data are descirbed in the comments in the cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "path='your/path/to/data'\n",
    "path='/Users/reneehlozek/Code/CMBS4/'\n",
    "low_Z_spec_galaxies = np.loadtxt(path+\"SDSS_spextral_galaxies_z0.1to0.2_wide.csv\",delimiter = \",\", skiprows=2,usecols=(3,12,13,14,15,16,19,20))\n",
    "mid_Z_spec_galaxies = np.loadtxt(path+\"SDSS_spextral_galaxies_z0.4to0.5_wide.csv\",delimiter = \",\", skiprows=2,usecols=(3,12,13,14,15,16,19,20))\n",
    "high_Z_spec_galaxies = np.loadtxt(path+\"SDSS_spextral_galaxies_z0.6to0.9_wide.csv\",delimiter = \",\", skiprows=2,usecols=(3,12,13,14,15,16,19,20))\n",
    "\n",
    "\n",
    "## column ids\n",
    "# 0  z\n",
    "# 1  ra\n",
    "# 2  dec\n",
    "# 3  r magnitude   (red)\n",
    "# 4  u magnitude   (Ultra Violet)\n",
    "# 5  g magnitude   (green)\n",
    "# 6  u-g color\n",
    "# 7  g-r color (redness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with and visualizing the data\n",
    "\n",
    "### Spatial distribution of objects\n",
    "\n",
    "As a first check, let's plot the spatial distributions of objects in our sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(low_Z_spec_galaxies[:,1],low_Z_spec_galaxies[:,2],color=\"b\", linestyle='None', marker='o')\n",
    "plt.plot(mid_Z_spec_galaxies[:,1],mid_Z_spec_galaxies[:,2],color=\"g\", linestyle='None', marker='d')\n",
    "plt.plot(high_Z_spec_galaxies[:,1],high_Z_spec_galaxies[:,2],color=\"r\", linestyle='None', marker='*')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the computational challenge of carrying out our analysis, let us select a subset of these data on a square patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## write a fuction ot select objects in a square patch\n",
    "def select_objects_in_radec_range(map_,ra_min,ra_max,dec_min,dec_max):\n",
    "    dec_ok = np.logical_and(map_[:,1] > ra_min,map_[:,1] < ra_max)\n",
    "    ra_ok  = np.logical_and(map_[:,2] > dec_min,map_[:,2] < dec_max)\n",
    "    ok = np.where(np.logical_and(dec_ok,ra_ok))\n",
    "    return(np.ravel(ok))\n",
    "\n",
    "## set boundaries for the square\n",
    "ra_min = 150\n",
    "ra_max = 170\n",
    "dec_min = 0\n",
    "dec_max = 20\n",
    "\n",
    "## come up with arrayse of indexes for each of the three redshift slices\n",
    "ok_low_Z = select_objects_in_radec_range(low_Z_spec_galaxies,ra_min,ra_max,dec_min,dec_max)\n",
    "ok_mid_Z = select_objects_in_radec_range(mid_Z_spec_galaxies,ra_min,ra_max,dec_min,dec_max)\n",
    "ok_high_Z = select_objects_in_radec_range(high_Z_spec_galaxies,ra_min,ra_max,dec_min,dec_max)\n",
    "\n",
    "\n",
    "## plot the spatial distribution of galaxies in these tiles\n",
    "\n",
    "plt.plot(low_Z_spec_galaxies[ok_low_Z,1],low_Z_spec_galaxies[ok_low_Z,2],\"b,\")\n",
    "plt.title(\"low redshift sample\")\n",
    "plt.xlabel(\"RA (deg)\")\n",
    "plt.ylabel(\"DEC (deg)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "plt.plot(mid_Z_spec_galaxies[ok_mid_Z,1],mid_Z_spec_galaxies[ok_mid_Z,2],\"g,\")\n",
    "plt.title(\"mid redshift sample\")\n",
    "plt.xlabel(\"RA (deg)\")\n",
    "plt.ylabel(\"DEC (deg)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "plt.plot(high_Z_spec_galaxies[ok_high_Z,1],high_Z_spec_galaxies[ok_high_Z,2],\"r,\")\n",
    "plt.title(\"high redshift sample\")\n",
    "plt.xlabel(\"RA (deg)\")\n",
    "plt.ylabel(\"DEC (deg)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "1. Are these objects randomly distributed? [Make a histogram of the objects] Think about the structures seen in the low redshift sample in particular.\n",
    "\n",
    "2. Are the distributions of objects in the low, mid, and high redshfits maps the same?  Think about why you would expect the stuctures not to be common.  Discuss whether the clumpiness is the same or different and why you would expect that to be different in the different maps.  (Note, ignore the strange feature in the upper left of the high redshift sample for this question.)\n",
    "\n",
    "3. What do you think the strange feature in the upper left of the high redshfit sample is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redshift evolution\n",
    "\n",
    "It is obvious from the previous excercise (and set of plots) that the distribution of these objects evolves with redshift.  One advantage of these spectroscopic data is that they have a resolution in redshfit with an accuracy better than one part in 1500.  (This accuracy is called R).  Therefore it is possible to measure the distribution of matter along the line of sight as well as in the spatial dimensions.  \n",
    "\n",
    "Here we plot the distribution of galaxies in the RA/redshift plane, but fixing a single Dec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(low_Z_spec_galaxies[ok_low_Z,1],low_Z_spec_galaxies[ok_low_Z,0],\"b,\")\n",
    "plt.title(\"low redshfit sample\")\n",
    "plt.xlabel(\"RA (deg)\")\n",
    "plt.ylabel(\"redshift (z)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(mid_Z_spec_galaxies[ok_mid_Z,1],mid_Z_spec_galaxies[ok_mid_Z,0],\"g,\")\n",
    "plt.title(\"mid redshfit sample\")\n",
    "plt.xlabel(\"RA (deg)\")\n",
    "plt.ylabel(\"redshift (z)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(high_Z_spec_galaxies[ok_high_Z,1],high_Z_spec_galaxies[ok_high_Z,0],\"r,\")\n",
    "plt.title(\"high redshfit sample\")\n",
    "plt.xlabel(\"RA (deg)\")\n",
    "plt.ylabel(\"redshift (z)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "1. What evolution do you see in the low redshift slice.  Based on this, is the redshift range too large or about right for measuring the spatial variations in LSS?\n",
    "\n",
    "2. Do you see any strange features in the high redshift slice?  What do you think is causing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object selection\n",
    "\n",
    "One other feature in the data is the nature of the objects.   The objects we have been considering have a \"galaxy calssification\".  This classification is based on immaging an separating things that look like point sources and things that are diffuse.   This separation is not perfect, and some diffuse obects can be related to instrument effects or other astrophsyical objets.   One way to probe the nature of the objects is to evaluate the relationship between their color and magnitude.  Magnitude is the intensity of the light from the object.  This can be bolometric intensity (which is the brightness as observed) or the absolute mangitude which is the intensity with a correction for distance (redshift) so it quantifes the intensity of the light emitted from the galxay.  Intensity is measured on a logarithmic scale with *negative* intensity corrresponding to brighter galaxies than *positive intensity*. Color is quantified by measurin the intensity in different bands and taking the difference.   \n",
    "\n",
    "In the data set provided, we have the red (r), UV (u), ang green (g) magnitudes, and colors (u-g) and (g-r).  Here we plot the r magnitude vs the g-r color for the low redshfit sample.\n",
    "\n",
    "\n",
    "The SDSS Filter throughput (source https://www.sdss.org/instruments/camera/)\n",
    "<div>\n",
    "<img src=\"https://www.sdss.org/wp-content/uploads/2014/11/camera_filters-300x274.jpg\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(low_Z_spec_galaxies[:,3],low_Z_spec_galaxies[:,7],\"b,\")\n",
    "plt.ylim(-20,20)\n",
    "plt.xlabel('r magnitude', fontsize=20)\n",
    "plt.ylabel('g-r color', fontsize=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "Are all the objects in this plot well behaved galaxies?   Do you see any evidence for systematic effects?   Do you see evidence for multiple galaxy populations of objects?   How might you separate these populations for an analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to quantify the level correlation (which is clearly visible by eye) in this data set.  We will focus on the two point angular correlation rather than the 3-d two point correlation which is slightly tricker to implement. This analysis is eqivelent, and mathematically related to a power spectrum analysis.\n",
    " \n",
    "\n",
    "First we write code to generate a random distribution of objects.  We will use this as point of comparison for our quantitative treatment.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function to generate ra and dec coordinates for N random objects \n",
    "#in a square box in RA and DEC\n",
    "def generate_random_ra_dec(N,ra_min, ra_max, dec_min, dec_max):\n",
    "    ra = np.random.uniform(low = ra_min, high = ra_max, size = N)\n",
    "    dec = np.random.uniform(low = dec_min, high = dec_max, size = N)\n",
    "    return(ra,dec)\n",
    "\n",
    "## generate a randomly distribtued set of objects in the same RA, \n",
    "## DEC range, and with the same number of objets as our low Z sample\n",
    "rand_ra,rand_dec = generate_random_ra_dec(np.size(low_Z_spec_galaxies[ok_low_Z,1]),\n",
    "                                          ra_min, ra_max, dec_min, dec_max)\n",
    "\n",
    "\n",
    "## plot this randomly distribtued simulation\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(rand_ra,rand_dec,\"k,\")\n",
    "plt.title(\"randomly distriibuted objects\")\n",
    "plt.xlabel(\"RA (deg)\")\n",
    "plt.ylabel(\"DEC (deg)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "Compare this to the plot above of the low-z sample.  What is different?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now develop the angular two point correlation function.  This function is the probability of finding galxies  as a function of their angular seperation.   This can be computed with a simple and ubqiutous tool, the histogram.   Our first job is to compute the histogram of the separation of these data.  \n",
    "\n",
    "Below is a function that carries this out.  This function takes in two sets of inputs $ra_1$, $dec_1$, and $ra_2$, $dec_2$.   If we pass the same data in as for the first and second data sets then it computes the two point correlation with itself, the *auto-correlation*.  If we pass differnet data sets then it computes the cross-correlation.  We will use this to compute the correct two point correlation below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# code to compute the pair counts (histogram)\n",
    "\n",
    "def pair_counts(ra1,dec1,ra2,dec2,histogram_bins):\n",
    "    ra1  = np.ravel(ra1)\n",
    "    dec1 = np.ravel(dec1)\n",
    "    ra2  = np.ravel(ra2)\n",
    "    dec2 = np.ravel(dec2)\n",
    "    N = np.size(ra1)\n",
    "    i = 0\n",
    "    while (i < N):\n",
    "        ra_cur= ra1[i]\n",
    "        dec_cur = dec1[i]\n",
    "        distance = np.sqrt((ra2-ra_cur)**2 + (dec2 - dec_cur)**2)\n",
    "        ok = np.where(distance > 0.01)\n",
    "        distance = distance[ok]\n",
    "        if i == 0:\n",
    "            histogram_out, bin_edges = np.histogram(distance,bins = histogram_bins)\n",
    "            print(\"initilized binning\")\n",
    "        histogram_cur, bin_edges = np.histogram(distance,bins = histogram_bins)\n",
    "        histogram_out += histogram_cur\n",
    "        i+=1\n",
    "    return(histogram_out,bin_edges)\n",
    "\n",
    "## seting up the bins for the pair count\n",
    "histogram_bins = np.logspace(-1.8,1)\n",
    "\n",
    "# choosing the data to pass into the pair counts\n",
    "data_ra  = low_Z_spec_galaxies[ok_low_Z,1]\n",
    "data_dec = low_Z_spec_galaxies[ok_low_Z,2]\n",
    "\n",
    "\n",
    "\n",
    "## calculate the pair counts for the Data\n",
    "DD,bins = pair_counts(data_ra,data_dec,data_ra,data_dec,histogram_bins)\n",
    "\n",
    "\n",
    "## plot the pair count histogram for the Data\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.loglog(bins[1:],DD)\n",
    "plt.title(\"pair counts (data)\")\n",
    "plt.xlabel(\"angular separation (degrees)\",fontsize=20)\n",
    "plt.ylabel(\"counts (arb)\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "Compute the pair count histogram for the random field and compare it (by plotting both on the same axes) to the histogram from the data. What difference do you note between theses two pair counts? How does this vary with angle?  Does this match your expectations from looking at the maps?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the a correlation function such that purely random data gives no correlation.  This analysis follows [Connolly et al. 2001](https://arxiv.org/abs/astro-ph/0107417)\n",
    "\n",
    "We use the estimator presented in Landy & Szalay (1993)(http://articles.adsabs.harvard.edu/full/1993ApJ...412...64L). \n",
    "\n",
    "$$W(\\theta) = \\frac{DD - 2DR + RR }{RR}$$\n",
    "\n",
    "Here DD is the data autocorrelation (above), RR is the random autocorrelation, and DR is the data-random correlation.  This is computed by using the RA, DEC from the data as the first argument to the pair_counts function and the RA, DEC from the randoms as the second.   \n",
    "\n",
    "Note: if both data sets are random you would expect this function to return zero correlation.\n",
    "\n",
    "The Landy & Szalay estimator is helpful for dealing with nonidealities (e.g., holes, masking, edge effects) in realistic survey data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "Test that the correlation funtion returns zero if both data sets are random. **NOTE** you must generate two random data sets for this to be a non-trivial test (discuss why).  Call these sets rand_ra2,rand_dec2 to avoid confusion with the true data.  Plot this with \"semilogx\" (eg., linear y-axis) since we expect the result to scatter around zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "Compute the correlation function for the data and plot on log-log plots.  Compare this to [Figure 1](https://arxiv.org/pdf/astro-ph/0107417.pdf#page=7).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "1. Did you recover the correct slope of the power spectrum?\n",
    "\n",
    "2. If the probability exceeds random at some scales, it must be below random on others.  Thus the dip at a few degree scales is real and corresponds to voids.\n",
    "\n",
    "3. Did you recover the correct normalization?  If not, why might it be off?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> EXCERCISE: </font> \n",
    "    \n",
    "Compute the angular two point function from a higher redshift slice.   Plot the low and high-redshift results aginst each other. Do they agree or not?  Why might explain the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>EXCERCISE: </font> \n",
    "\n",
    "Now divide the samples into several different bins based on the galaxy magnitudes. \n",
    "Compute and plot the correlation functions. What trends do you see? What is the cause of this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your discussion here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "With high quality data it is possible to measure the BAO peak (150 Mpc, ~2 degrees at z = 2) with the angular correlation function.  However, a better method is to compute the three dimensional power correlation.  To do this one needs to convert the positional information to spatial cooordinates which requires accounting for distance (redshift) and the cosmological evolution of the scale factor. Once the data are in spatial coordinates (eg., Mpc for X,Y,Z) then one can compute the 3d correlation using methods nearly identical to those presented above.  This is left as an optional and advanced excercise to the stduent.\n",
    "\n",
    "We hope this exercise has given you familarity with the evolution of large scale structure as it can be seen in spectroscopic data.  We also hope we have demystified the 2pt correlation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
